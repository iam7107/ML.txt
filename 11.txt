# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Load the Boston Housing dataset
# Replace with local dataset path if needed
url = "https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv"
data = pd.read_csv(url)

# Display the first few rows of the dataset
print("First few rows of the dataset:")
print(data.head())

# Step 1: Select features and target variable
# Using 'rm' as the feature for simplicity in visualization. You can add more features if needed.
X = data[['rm']]
y = data['medv']

# Step 2: Transform features into polynomial terms
degree = 2  # Degree of the polynomial (change to 3 or higher if needed)
poly_features = PolynomialFeatures(degree=degree)
X_poly = poly_features.fit_transform(X)

# Step 3: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)
# Step 4: Train the Polynomial Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Step 5: Make predictions on the test data
y_pred = model.predict(X_test)

# Step 6: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Model Performance:")
print(f"Mean Squared Error: {mse}")
print(f"R-squared Score: {r2}")

# Step 7: Visualize the Polynomial Regression curve
plt.figure(figsize=(10, 6))
plt.scatter(X['rm'], y, color='blue', label='Actual Prices')

# Sort values for a smooth curve
X_sorted, y_pred_sorted = zip(*sorted(zip(X['rm'], model.predict(poly_features.transform(X)))))
plt.plot(X_sorted, y_pred_sorted, color='red', label='Polynomial Regression Curve')
plt.xlabel('Average Number of Rooms (rm)')
plt.ylabel('Median Price (medv) in $1000s')
plt.title(f'Polynomial Regression (Degree {degree})')
plt.legend()
plt.show()









#q2
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report,confusion_matrix
from sklearn import tree
import matplotlib.pyplot as plt
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt"
column_names = ['Variance', 'Skewness', 'Curtosis', 'Entropy', 'Class']
df = pd.read_csv(url, header=None, names=column_names)

# Step 3: Preprocess the Data
# Extract features (X) and target variable (y)
X = df.drop('Class', axis=1).values # Features
y = df['Class'].values # Target (0 for forged, 1 for genuine)

# Step 4: Split the Data into Training and Testing Sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)

# Step 5: Train the Decision Tree Classifier
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train, y_train)

# Step 6: Evaluate the Model
# Predictions on the test set
y_pred = dt_classifier.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

# Print the classification report
print("Classification Report:\n", classification_report(y_test, y_pred))

# Print the confusion matrix
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# Step 7: Visualize the Decision Tree (Optional)
plt.figure(figsize=(20,10))
tree.plot_tree(dt_classifier, feature_names=column_names[:-1],
class_names=['Forged', 'Genuine'], filled=True)
plt.show()
