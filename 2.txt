import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Load the dataset (Example dataset URL: replace with actual URL if required)
url = "https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv"
data = pd.read_csv(url)

# Display the first few rows
print("First few rows of the dataset:")
print(data.head())

# Check for null values
print("\nChecking for null values in each column:")
print(data.isnull().sum())

# Remove rows with any null values
data = data.dropna()

# Split data into features and target variable
X = data[['rm']]  # Using the 'rm' column (average number of rooms per dwelling) as a feature
y = data['medv']  # 'medv' is the median value of owner-occupied homes in $1000s

# Split dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a linear regression model and fit it to the training data
model = LinearRegression()
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("\nModel Performance:")
print(f"Mean Squared Error: {mse}")
print(f"R^2 Score: {r2}")

# Plotting the regression line on the test data
plt.scatter(X_test, y_test, color='blue', label='Actual House Prices')
plt.plot(X_test, y_pred, color='red', linewidth=2, label='Predicted Regression Line')
plt.xlabel("Average Number of Rooms per Dwelling (rm)")
plt.ylabel("Median Value of Homes (medv)")
plt.title("Simple Linear Regression: House Price Prediction")
plt.legend()
plt.show()





#q2
# Import necessary libraries
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score
import scipy.cluster.hierarchy as sch
import matplotlib.pyplot as plt

# 1. Load the dataset
url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv'
wholesale_data = pd.read_csv(url)

# Display the first few rows
print("First few rows of the dataset:")
print(wholesale_data.head())

# 2. Check for null values and drop them if necessary
print("\nChecking for missing values:")
print(wholesale_data.isnull().sum())

# Dropping rows with missing values (if any)
wholesale_data = wholesale_data.dropna()

# 3. Normalize the spending data
scaler = StandardScaler()
spending_data = wholesale_data.iloc[:, 2:]  # Select only spending columns (skip 'Channel' and 'Region')
scaled_data = scaler.fit_transform(spending_data)

# 4. Perform Agglomerative Clustering (updated to use 'metric' instead of 'affinity')
agg_clustering = AgglomerativeClustering(n_clusters=3, metric='euclidean', linkage='ward')
clusters = agg_clustering.fit_predict(scaled_data)

# Add cluster labels to the original dataset
wholesale_data['Cluster'] = clusters
print("\nClustered Data:")
print(wholesale_data.head())

# 5. Visualize the dendrogram
plt.figure(figsize=(10, 7))
dendrogram = sch.dendrogram(sch.linkage(scaled_data, method='ward'))
plt.title('Dendrogram')
plt.xlabel('Customers')
plt.ylabel('Euclidean distances')
plt.show()

# 6. Evaluate the clustering with Silhouette Score
score = silhouette_score(scaled_data, clusters)
print(f'\nSilhouette Score: {score}')
