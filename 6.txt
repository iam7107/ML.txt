# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Load the Boston Housing dataset
# Replace with local dataset path if needed
url = "https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv"
data = pd.read_csv(url)

# Display the first few rows of the dataset
print("First few rows of the dataset:")
print(data.head())

# Step 1: Select features and target variable
# Using 'rm' as the feature for simplicity in visualization. You can add more features if needed.
X = data[['rm']]
y = data['medv']

# Step 2: Transform features into polynomial terms
degree = 2  # Degree of the polynomial (change to 3 or higher if needed)
poly_features = PolynomialFeatures(degree=degree)
X_poly = poly_features.fit_transform(X)

# Step 3: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)

# Step 4: Train the Polynomial Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Step 5: Make predictions on the test data
y_pred = model.predict(X_test)

# Step 6: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Model Performance:")
print(f"Mean Squared Error: {mse}")
print(f"R-squared Score: {r2}")

# Step 7: Visualize the Polynomial Regression curve
plt.figure(figsize=(10, 6))
plt.scatter(X['rm'], y, color='blue', label='Actual Prices')

# Sort values for a smooth curve
X_sorted, y_pred_sorted = zip(*sorted(zip(X['rm'], model.predict(poly_features.transform(X)))))
plt.plot(X_sorted, y_pred_sorted, color='red', label='Polynomial Regression Curve')
plt.xlabel('Average Number of Rooms (rm)')
plt.ylabel('Median Price (medv) in $1000s')
plt.title(f'Polynomial Regression (Degree {degree})')
plt.legend()
plt.show()











#q2
# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# Load the dataset
# Replace 'employee_data.csv' with your local path to the dataset
data = pd.read_csv('employee_data.csv')

# Display the first few rows of the dataset
print(data.head())

# Check for missing values
print(data.isnull().sum())

# Step 2: Preprocess the data
# Drop missing values (if any)
data = data.dropna()

# Select features for clustering (Experience and Salary)
X = data[['Experience (Years)', 'Salary']]

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 3: Use the Elbow Method to find the optimal number of clusters
inertia = []
silhouette_scores = []

# Testing k values from 2 to 10
k_values = range(2, 11)
for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    
# Calculate inertia
    inertia.append(kmeans.inertia_)
    
# Calculate silhouette score
    score = silhouette_score(X_scaled, kmeans.labels_)
    silhouette_scores.append(score)

# Plotting the Elbow Method
plt.figure(figsize=(14, 6))

# Elbow Method Plot
plt.subplot(1, 2, 1)
plt.plot(k_values, inertia, marker='o')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Inertia')
plt.xticks(k_values)

# Silhouette Score Plot
plt.subplot(1, 2, 2)
plt.plot(k_values, silhouette_scores, marker='o', color='orange')
plt.title('Silhouette Score for Optimal k')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Silhouette Score')
plt.xticks(k_values)

plt.tight_layout()
plt.show()

# Choose optimal k based on elbow and silhouette score
optimal_k = 4  # Replace this with your analysis result from the plots

# Step 4: Fit K-means model with optimal k
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
clusters = kmeans.fit_predict(X_scaled)

# Add cluster labels to the original dataset
data['Cluster'] = clusters

# Display clustered data
print("\nClustered Data:")
print(data.head())


# Visualize the clusters
plt.figure(figsize=(10, 8))
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap='viridis')
plt.title('K-means Clustering of Employees')
plt.xlabel('Experience (Years) (scaled)')
plt.ylabel('Salary (scaled)')
plt.colorbar(label='Cluster')
plt.grid()
plt.show()
